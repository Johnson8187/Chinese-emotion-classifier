# chinese-text-emotion-classifier

## ğŸ“š Model Introduction

This model is fine-tuned based on the [joeddav/xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli) model, specializing in **Chinese text emotion analysis**.  
Through fine-tuning, the model can identify the following 8 emotion labels:
- **Neutral tone**
- **Concerned tone**
- **Happy tone**
- **Angry tone**
- **Sad tone**
- **Questioning tone**
- **Surprised tone**
- **Disgusted tone**

The model is applicable to various scenarios, such as customer service emotion monitoring, social media analysis, and user feedback classification.

---
# chinese-text-emotion-classifier

## ğŸ“š æ¨¡å‹ç°¡ä»‹
æœ¬æ¨¡å‹åŸºæ–¼[joeddav/xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli) æ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œå°ˆæ³¨æ–¼ **ä¸­æ–‡èªå¥æƒ…æ„Ÿåˆ†æ**ã€‚  
é€šéå¾®èª¿ï¼Œæ¨¡å‹å¯ä»¥è­˜åˆ¥ä»¥ä¸‹ 8 ç¨®æƒ…ç·’æ¨™ç±¤ï¼š
- **å¹³æ·¡èªæ°£**
- **é—œåˆ‡èªèª¿**
- **é–‹å¿ƒèªèª¿**
- **æ†¤æ€’èªèª¿**
- **æ‚²å‚·èªèª¿**
- **ç–‘å•èªèª¿**
- **é©šå¥‡èªèª¿**
- **å­æƒ¡èªèª¿**

è©²æ¨¡å‹é©ç”¨æ–¼å¤šç¨®å ´æ™¯ï¼Œä¾‹å¦‚å®¢æœæƒ…ç·’ç›£æ§ã€ç¤¾äº¤åª’é«”åˆ†æä»¥åŠç”¨æˆ¶åé¥‹åˆ†é¡ã€‚

---
## ğŸš€ Quick Start

### Install Dependencies
Ensure that you have installed Hugging Face's Transformers library and PyTorch:
```bash
pip install transformers torch
```

###Load the Model
Use the following code to load the model and tokenizer, and perform emotion classification:
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# æ·»åŠ è¨­å‚™è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ¨™ç±¤æ˜ å°„å­—å…¸
label_mapping = {
    0: "å¹³æ·¡èªæ°£",
    1: "é—œåˆ‡èªèª¿",
    2: "é–‹å¿ƒèªèª¿",
    3: "æ†¤æ€’èªèª¿",
    4: "æ‚²å‚·èªèª¿",
    5: "ç–‘å•èªèª¿",
    6: "é©šå¥‡èªèª¿",
    7: "å­æƒ¡èªèª¿"
}

def predict_emotion(text, model_path="Johnson8187/Chinese-emotion-classifier"):
    # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)  # ç§»å‹•æ¨¡å‹åˆ°è¨­å‚™
    
    # å°‡æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)  # ç§»å‹•è¼¸å…¥åˆ°è¨­å‚™
    
    # é€²è¡Œé æ¸¬
    with torch.no_grad():
        outputs = model(**inputs)
    
    # å–å¾—é æ¸¬çµæœ
    predicted_class = torch.argmax(outputs.logits).item()
    predicted_emotion = label_mapping[predicted_class]
    
    return predicted_emotion

if __name__ == "__main__":
    # ä½¿ç”¨ç¯„ä¾‹
    test_texts = [
        "é›–ç„¶æˆ‘åŠªåŠ›äº†å¾ˆä¹…ï¼Œä½†ä¼¼ä¹ç¸½æ˜¯åšä¸åˆ°ï¼Œæˆ‘æ„Ÿåˆ°è‡ªå·±ä¸€ç„¡æ˜¯è™•ã€‚",
        "ä½ èªªçš„é‚£äº›è©±çœŸçš„è®“æˆ‘å¾ˆå›°æƒ‘ï¼Œå®Œå…¨ä¸çŸ¥é“è©²æ€éº¼åæ‡‰ã€‚",
        "é€™ä¸–ç•ŒçœŸçš„æ˜¯ç„¡æƒ…ï¼Œç‚ºä»€éº¼æ¯æ¬¡éƒ½è¦çµ¦æˆ‘é€™æ¨£çš„è€ƒé©—ï¼Ÿ",
        "æœ‰æ™‚å€™ï¼Œæˆ‘åªå¸Œæœ›èƒ½æœ‰ä¸€é»å®‰éœï¼Œä¸è¦å†è½åˆ°é€™äº›ç„¡èŠçš„è©±é¡Œã€‚",
        "æ¯æ¬¡æƒ³èµ·é‚£æ®µéå»ï¼Œæˆ‘çš„å¿ƒé‚„æ˜¯æœƒç—›ï¼ŒçœŸçš„ç„¡æ³•é‡‹æ‡·ã€‚",
        "æˆ‘å¾ä¾†æ²’æœ‰æƒ³éæœƒæœ‰é€™éº¼å¤§çš„æ”¹è®Šï¼Œç¾åœ¨æˆ‘è¦ºå¾—è‡ªå·±å®Œå…¨å¤±æ§äº†ã€‚",
        "æˆ‘å®Œå…¨æ²’æƒ³åˆ°ä½ æœƒé€™éº¼åšï¼Œé€™è®“æˆ‘é©šè¨åˆ°ç„¡æ³•è¨€å–»ã€‚",
        "æˆ‘çŸ¥é“æˆ‘æ‡‰è©²æ›´å …å¼·ï¼Œä½†æœ‰äº›æ™‚å€™ï¼Œé€™ç¨®æƒ…ç·’çœŸçš„è®“æˆ‘å¿«è¦å´©æ½°äº†ã€‚"
    ]

    for text in test_texts:
        emotion = predict_emotion(text)
        print(f"æ–‡æœ¬: {text}")
        print(f"é æ¸¬æƒ…ç·’: {emotion}\n")

```

---
## ğŸš€ å¿«é€Ÿé–‹å§‹

### å®‰è£ä¾è³´
è«‹ç¢ºä¿å®‰è£äº† Hugging Face çš„ Transformers åº«å’Œ PyTorchï¼š
```bash
pip install transformers torch
```

### åŠ è¼‰æ¨¡å‹
ä½¿ç”¨ä»¥ä¸‹ä»£ç¢¼åŠ è¼‰æ¨¡å‹å’Œåˆ†è©å™¨ï¼Œä¸¦é€²è¡Œæƒ…æ„Ÿåˆ†é¡ï¼š
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# æ·»åŠ è¨­å‚™è¨­å®š
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ¨™ç±¤æ˜ å°„å­—å…¸
label_mapping = {
    0: "å¹³æ·¡èªæ°£",
    1: "é—œåˆ‡èªèª¿",
    2: "é–‹å¿ƒèªèª¿",
    3: "æ†¤æ€’èªèª¿",
    4: "æ‚²å‚·èªèª¿",
    5: "ç–‘å•èªèª¿",
    6: "é©šå¥‡èªèª¿",
    7: "å­æƒ¡èªèª¿"
}

def predict_emotion(text, model_path="Johnson8187/Chinese-emotion-classifier"):
    # è¼‰å…¥æ¨¡å‹å’Œåˆ†è©å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)  # ç§»å‹•æ¨¡å‹åˆ°è¨­å‚™
    
    # å°‡æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)  # ç§»å‹•è¼¸å…¥åˆ°è¨­å‚™
    
    # é€²è¡Œé æ¸¬
    with torch.no_grad():
        outputs = model(**inputs)
    
    # å–å¾—é æ¸¬çµæœ
    predicted_class = torch.argmax(outputs.logits).item()
    predicted_emotion = label_mapping[predicted_class]
    
    return predicted_emotion

if __name__ == "__main__":
    # ä½¿ç”¨ç¯„ä¾‹
    test_texts = [
        "é›–ç„¶æˆ‘åŠªåŠ›äº†å¾ˆä¹…ï¼Œä½†ä¼¼ä¹ç¸½æ˜¯åšä¸åˆ°ï¼Œæˆ‘æ„Ÿåˆ°è‡ªå·±ä¸€ç„¡æ˜¯è™•ã€‚",
        "ä½ èªªçš„é‚£äº›è©±çœŸçš„è®“æˆ‘å¾ˆå›°æƒ‘ï¼Œå®Œå…¨ä¸çŸ¥é“è©²æ€éº¼åæ‡‰ã€‚",
        "é€™ä¸–ç•ŒçœŸçš„æ˜¯ç„¡æƒ…ï¼Œç‚ºä»€éº¼æ¯æ¬¡éƒ½è¦çµ¦æˆ‘é€™æ¨£çš„è€ƒé©—ï¼Ÿ",
        "æœ‰æ™‚å€™ï¼Œæˆ‘åªå¸Œæœ›èƒ½æœ‰ä¸€é»å®‰éœï¼Œä¸è¦å†è½åˆ°é€™äº›ç„¡èŠçš„è©±é¡Œã€‚",
        "æ¯æ¬¡æƒ³èµ·é‚£æ®µéå»ï¼Œæˆ‘çš„å¿ƒé‚„æ˜¯æœƒç—›ï¼ŒçœŸçš„ç„¡æ³•é‡‹æ‡·ã€‚",
        "æˆ‘å¾ä¾†æ²’æœ‰æƒ³éæœƒæœ‰é€™éº¼å¤§çš„æ”¹è®Šï¼Œç¾åœ¨æˆ‘è¦ºå¾—è‡ªå·±å®Œå…¨å¤±æ§äº†ã€‚",
        "æˆ‘å®Œå…¨æ²’æƒ³åˆ°ä½ æœƒé€™éº¼åšï¼Œé€™è®“æˆ‘é©šè¨åˆ°ç„¡æ³•è¨€å–»ã€‚",
        "æˆ‘çŸ¥é“æˆ‘æ‡‰è©²æ›´å …å¼·ï¼Œä½†æœ‰äº›æ™‚å€™ï¼Œé€™ç¨®æƒ…ç·’çœŸçš„è®“æˆ‘å¿«è¦å´©æ½°äº†ã€‚"
    ]

    for text in test_texts:
        emotion = predict_emotion(text)
        print(f"æ–‡æœ¬: {text}")
        print(f"é æ¸¬æƒ…ç·’: {emotion}\n")

```

---

### Dataset
- The fine-tuning dataset consists of 4,000 annotated Traditional Chinese emotion samples, covering various emotion categories to ensure the model's generalization capability in emotion classification.
- [Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset](https://huggingface.co/datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset)


### æ•¸æ“šé›†
- å¾®èª¿æ•¸æ“šä¾†è‡ª4000å€‹è‡ªè¡Œæ¨™è¨»çš„é«˜è³ªé‡ç¹é«”ä¸­æ–‡æƒ…æ„Ÿèªå¥æ•¸æ“šï¼Œè¦†è“‹äº†å¤šç¨®æƒ…ç·’é¡åˆ¥ï¼Œç¢ºä¿æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†é¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚
- [Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset](https://huggingface.co/datasets/Johnson8187/Chinese_Multi-Emotion_Dialogue_Dataset)

---

ğŸŒŸ Contact and Feedback
If you encounter any issues while using this model, please contact:

Email: fable8043@gmail.com
Hugging Face Project Page: chinese-text-emotion-classifier

## ğŸŒŸ è¯ç¹«èˆ‡åé¥‹
å¦‚æœæ‚¨åœ¨ä½¿ç”¨è©²æ¨¡å‹æ™‚æœ‰ä»»ä½•å•é¡Œï¼Œè«‹è¯ç¹«ï¼š
- éƒµç®±ï¼š`fable8043@gmail.com`
- Hugging Face é …ç›®é é¢ï¼š[chinese-text-emotion-classifier](https://huggingface.co/Johnson8187/chinese-text-emotion-classifier)
